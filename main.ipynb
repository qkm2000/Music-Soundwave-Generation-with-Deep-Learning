{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "This model will take in a valence-arousal vector and a Bouba-Kiki vector. It will then output an encoded earcon representation, which will be passed to the MusicGen Decoder to generate the final earcon.\n",
    "\n",
    "The output of the MusicGen Decoder will then be encoded by EncodecFeatureExtractor, and the vectors will be used to calculate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataset will consist of:\n",
    "- An Earcon represented by an Encodec vector\n",
    "- An image represented in a Valence Arousal Vector\n",
    "- A Bouba-Kiki Value derived from the image\n",
    "- A Pseudoword\n",
    "\n",
    "The rows will be paired by cosine similarity between the Earcon's Encodec vector and the VA Vector from the image. The Bouba-Kiki Value and Pseudoword will be generated after the images are paired with the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in earcons\n",
    "earcons = pd.read_csv('dataset\\earcon_dataset\\earcon_dataset.csv')\n",
    "\n",
    "earcons['query'] = earcons['query'].apply(ast.literal_eval)\n",
    "earcons[\"query\"] = earcons[\"query\"].apply(lambda x: x[0])\n",
    "\n",
    "earcons = earcons[[\"query\", \"name\"]]\n",
    "\n",
    "earcons[\"filepaths\"] = earcons[\"name\"].apply(lambda x: f\"dataset/earcon_dataset/earcons/{x}\")\n",
    "\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep encodec model\n",
    "from encodec import EncodecModel\n",
    "\n",
    "\n",
    "encodec_model = EncodecModel.encodec_model_24khz().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "def extract_earcon_features(filepaths, encodec_model, target_sample_rate=24000, target_length=512):\n",
    "    earcon_features = []\n",
    "\n",
    "    for path in filepaths:\n",
    "        # load in the audio file\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        waveform = waveform.to(device)\n",
    "\n",
    "        # if stereo, convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True).to(device)\n",
    "\n",
    "        # resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate).to(device)\n",
    "            waveform = resampler(waveform).to(device)\n",
    "\n",
    "        # add batch dimension so that the shape is\n",
    "        # [1, 1, num_samples] because encodec\n",
    "        # expects that format\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        # encode the waveform\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = encodec_model.encode(waveform)\n",
    "            compressed_features = encoded_frames[0][0].to(device)  # Take the first codebook\n",
    "        \n",
    "        # truncate and pad\n",
    "        length = compressed_features.shape[2]\n",
    "        if length > target_length:\n",
    "            compressed_features = compressed_features[:, :, :target_length].to(device)\n",
    "        else:\n",
    "            pad = torch.zeros((compressed_features.shape[0], compressed_features.shape[1], target_length - length)).to(device)\n",
    "            compressed_features = torch.cat((compressed_features, pad), dim=2).to(device)\n",
    "\n",
    "        # remove the first dimension\n",
    "        compressed_features = compressed_features.squeeze(0)\n",
    "        earcon_features.append(compressed_features.to(\"cpu\"))\n",
    "\n",
    "    return earcon_features\n",
    "\n",
    "\n",
    "# Apply the function to all rows in the earcons dataframe\n",
    "earcons[\"earcon_features\"] = extract_earcon_features(earcons[\"filepaths\"], encodec_model)\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcons[\"earcon_features\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the largest value embedded in the earcon features\n",
    "\n",
    "smallest = 1000000\n",
    "largest = 0\n",
    "for i in range(len(earcons[\"earcon_features\"])):\n",
    "    if largest < int(earcons[\"earcon_features\"][i].max()):\n",
    "        largest = int(earcons[\"earcon_features\"][i].max())\n",
    "    if smallest > int(earcons[\"earcon_features\"][i].min()):\n",
    "        smallest = int(earcons[\"earcon_features\"][i].min())\n",
    "\n",
    "print(largest)\n",
    "print(smallest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in images\n",
    "images = pd.read_csv('dataset\\landscape1\\csvs\\image_classification.csv')\n",
    "\n",
    "# extract top tag and similarity score\n",
    "images['top_tags'] = images['top_tags'].apply(ast.literal_eval)\n",
    "images[\"top_tags\"] = images[\"top_tags\"].apply(lambda x: x[0])\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(ast.literal_eval)\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(lambda x: x[0])\n",
    "\n",
    "images[\"image_path\"] = images[\"image_path\"].str.lstrip(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CLIP model\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Function to calculate image vectors\n",
    "def calculate_image_vectors(image_paths, clip_model, clip_processor):\n",
    "    image_features = []\n",
    "    count = 0\n",
    "    for image_path in image_paths:\n",
    "        count += 1\n",
    "        image = clip_processor(images=Image.open(image_path), return_tensors=\"pt\").pixel_values.to(device)\n",
    "        with torch.no_grad():\n",
    "            image = clip_model.get_image_features(image)\n",
    "            print(image)\n",
    "            image = image.squeeze(0)\n",
    "            image_features.append(image.to(\"cpu\"))\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"Processed {count} images\")\n",
    "    return image_features\n",
    "\n",
    "\n",
    "# Apply the function to the images dataframe\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "images[\"image_features\"] = calculate_image_vectors(images[\"image_path\"].tolist(), clip_model, clip_processor)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[\"image_features\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity\n",
    "\n",
    "This will be used to build the dataset for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity & store in a new df\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_torch_cosine_similarity(image_vectors, earcon_vectors):\n",
    "    # Convert to PyTorch tensors\n",
    "    image_tensor = torch.tensor(np.stack(image_vectors))\n",
    "    earcon_tensor = torch.tensor(np.mean(np.stack(earcon_vectors), axis=1)).float()\n",
    "    \n",
    "    # Normalize vectors\n",
    "    image_tensor_norm = image_tensor / image_tensor.norm(dim=1, keepdim=True)\n",
    "    earcon_tensor_norm = earcon_tensor / earcon_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = torch.mm(image_tensor_norm, earcon_tensor_norm.t())\n",
    "    \n",
    "    return similarity_matrix.numpy()\n",
    "\n",
    "\n",
    "def process_similarities(images, earcons):\n",
    "    # Compute similarity matrix (using one of the methods above)\n",
    "    similarity_matrix = compute_torch_cosine_similarity(\n",
    "        images['image_features'].tolist(), \n",
    "        earcons['earcon_features'].tolist()\n",
    "    )\n",
    "    \n",
    "    # Find the index of the most similar earcon for each image\n",
    "    most_similar_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Extract the most similar earcon details\n",
    "    result_df = pd.DataFrame({\n",
    "        'split': images['split'],\n",
    "        'earcon_filename': earcons.iloc[most_similar_indices]['name'].values,\n",
    "        'earcon_filepath': earcons.iloc[most_similar_indices]['filepaths'].values,\n",
    "        'earcon_features': earcons.iloc[most_similar_indices]['earcon_features'].values,\n",
    "        'image_filename': images['filename'],\n",
    "        'image_filepath': images['image_path'],\n",
    "        'image_features': images['image_features'],\n",
    "        'image_tag': images['top_tags'],\n",
    "        'image_tag_similarity': images['similarity_scores'],\n",
    "        'similarity_score': similarity_matrix[np.arange(len(most_similar_indices)), most_similar_indices]\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "earcon_image_dataset = process_similarities(images, earcons)\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.earcon_filename.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pseudowords and Bouba-Kiki value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import utils.psword_gen as psword_gen\n",
    "import utils.psword_utils as psword_utils\n",
    "\n",
    "\n",
    "def generate_pseudoword_and_bouba_kiki(image_path, sound_dict):\n",
    "    x_values, y_values = psword_utils.process_image(image_path, 50, 150)\n",
    "    weighted_angles, roundness = psword_utils.calculate_weighted_angles_by_edge_length(x_values, y_values)\n",
    "    \n",
    "    random.seed(42)\n",
    "    \n",
    "    psword = psword_gen.pseudoword_generator(\n",
    "        roundness,\n",
    "        len(x_values),\n",
    "        sound_dict=sound_dict\n",
    "    )\n",
    "\n",
    "    roundness = torch.tensor(roundness).float()\n",
    "    \n",
    "    return roundness, psword, weighted_angles\n",
    "\n",
    "# Parallelized function\n",
    "def process_row(row):\n",
    "    return generate_pseudoword_and_bouba_kiki(row['image_filepath'], sound_dict)\n",
    "\n",
    "\n",
    "# sound dict\n",
    "sound_dict = psword_gen.load_sound_mappings('utils/sound_mappings.json')\n",
    "\n",
    "# Parallelize using joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(process_row)(row) for _, row in earcon_image_dataset.iterrows())\n",
    "\n",
    "# Extract and assign results\n",
    "earcon_image_dataset[['roundness', 'pseudoword', 'weighted_angles']] = pd.DataFrame(results, index=earcon_image_dataset.index)\n",
    "\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.to_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>earcon_filename</th>\n",
       "      <th>earcon_filepath</th>\n",
       "      <th>earcon_features</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>image_filepath</th>\n",
       "      <th>image_features</th>\n",
       "      <th>image_tag</th>\n",
       "      <th>image_tag_similarity</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>roundness</th>\n",
       "      <th>pseudoword</th>\n",
       "      <th>weighted_angles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>BS_Bend_20.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/BS_Bend_20.wav</td>\n",
       "      <td>[[tensor(930.), tensor(534.), tensor(530.), te...</td>\n",
       "      <td>Coast-Test (1).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.5427), tensor(-0.2114), tensor(-0.54...</td>\n",
       "      <td>a natural landscape</td>\n",
       "      <td>0.098083</td>\n",
       "      <td>0.059398</td>\n",
       "      <td>tensor(0.5484)</td>\n",
       "      <td>juxuluji</td>\n",
       "      <td>98.713105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>Failure_01.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Failure_01.wav</td>\n",
       "      <td>[[tensor(913.), tensor(945.), tensor(530.), te...</td>\n",
       "      <td>Coast-Test (100).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.1623), tensor(-0.0117), tensor(-0.17...</td>\n",
       "      <td>a natural landscape</td>\n",
       "      <td>0.120972</td>\n",
       "      <td>0.057352</td>\n",
       "      <td>tensor(0.5156)</td>\n",
       "      <td>juxulu</td>\n",
       "      <td>92.807444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>BS_Bend_17.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/BS_Bend_17.wav</td>\n",
       "      <td>[[tensor(319.), tensor(698.), tensor(857.), te...</td>\n",
       "      <td>Coast-Test (101).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.5334), tensor(-0.1099), tensor(-0.36...</td>\n",
       "      <td>a calm landscape</td>\n",
       "      <td>0.133911</td>\n",
       "      <td>0.072991</td>\n",
       "      <td>tensor(0.5630)</td>\n",
       "      <td>geleje</td>\n",
       "      <td>101.336326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>clock.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/clock.wav</td>\n",
       "      <td>[[tensor(62.), tensor(62.), tensor(62.), tenso...</td>\n",
       "      <td>Coast-Test (102).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.8153), tensor(-0.2710), tensor(0.351...</td>\n",
       "      <td>a bright landscape</td>\n",
       "      <td>0.113892</td>\n",
       "      <td>0.056997</td>\n",
       "      <td>tensor(0.5332)</td>\n",
       "      <td>juxulu</td>\n",
       "      <td>95.974916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Failure_01.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Failure_01.wav</td>\n",
       "      <td>[[tensor(913.), tensor(945.), tensor(530.), te...</td>\n",
       "      <td>Coast-Test (103).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.3290), tensor(-0.2365), tensor(-0.14...</td>\n",
       "      <td>a bright landscape</td>\n",
       "      <td>0.127563</td>\n",
       "      <td>0.066242</td>\n",
       "      <td>tensor(0.5516)</td>\n",
       "      <td>geleje</td>\n",
       "      <td>99.289937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>validation</td>\n",
       "      <td>Testeregitar_-_F.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Testeregitar_-_...</td>\n",
       "      <td>[[tensor(834.), tensor(604.), tensor(432.), te...</td>\n",
       "      <td>Mountain-Valid (95).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(-0.1323), tensor(0.4341), tensor(0.136...</td>\n",
       "      <td>a intense landscape</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.040474</td>\n",
       "      <td>tensor(0.5445)</td>\n",
       "      <td>juxuluji</td>\n",
       "      <td>98.015541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>validation</td>\n",
       "      <td>Anvil_-_Lokomo_A_100_kg_-_Hammer_on_back_1_tim...</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Anvil_-_Lokomo_...</td>\n",
       "      <td>[[tensor(170.), tensor(748.), tensor(748.), te...</td>\n",
       "      <td>Mountain-Valid (96).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(-0.3210), tensor(0.1672), tensor(0.141...</td>\n",
       "      <td>a complex landscape</td>\n",
       "      <td>0.069275</td>\n",
       "      <td>0.056088</td>\n",
       "      <td>tensor(0.5127)</td>\n",
       "      <td>juxuluja</td>\n",
       "      <td>92.280799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>validation</td>\n",
       "      <td>Short_Cut-Off_Beep.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Short_Cut-Off_B...</td>\n",
       "      <td>[[tensor(103.), tensor(56.), tensor(0.), tenso...</td>\n",
       "      <td>Mountain-Valid (97).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(0.3450), tensor(0.3041), tensor(0.0990...</td>\n",
       "      <td>a narrow landscape</td>\n",
       "      <td>0.117004</td>\n",
       "      <td>0.043641</td>\n",
       "      <td>tensor(0.5323)</td>\n",
       "      <td>juxuluji</td>\n",
       "      <td>95.818241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>validation</td>\n",
       "      <td>BS_Bend_17.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/BS_Bend_17.wav</td>\n",
       "      <td>[[tensor(319.), tensor(698.), tensor(857.), te...</td>\n",
       "      <td>Mountain-Valid (98).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(0.1858), tensor(-0.0550), tensor(-0.14...</td>\n",
       "      <td>a sharp landscape</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>tensor(0.5541)</td>\n",
       "      <td>gelejegi</td>\n",
       "      <td>99.729109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>validation</td>\n",
       "      <td>01530_censore_beep_05s.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/01530_censore_b...</td>\n",
       "      <td>[[tensor(1009.), tensor(205.), tensor(606.), t...</td>\n",
       "      <td>Mountain-Valid (99).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(0.0373), tensor(-0.1407), tensor(-0.11...</td>\n",
       "      <td>a positive landscape</td>\n",
       "      <td>0.071838</td>\n",
       "      <td>0.049314</td>\n",
       "      <td>tensor(0.5655)</td>\n",
       "      <td>gelejegi</td>\n",
       "      <td>101.785030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            split                                    earcon_filename  \\\n",
       "0            test                                     BS_Bend_20.wav   \n",
       "1            test                                     Failure_01.wav   \n",
       "2            test                                     BS_Bend_17.wav   \n",
       "3            test                                          clock.wav   \n",
       "4            test                                     Failure_01.wav   \n",
       "...           ...                                                ...   \n",
       "11995  validation                               Testeregitar_-_F.wav   \n",
       "11996  validation  Anvil_-_Lokomo_A_100_kg_-_Hammer_on_back_1_tim...   \n",
       "11997  validation                             Short_Cut-Off_Beep.wav   \n",
       "11998  validation                                     BS_Bend_17.wav   \n",
       "11999  validation                         01530_censore_beep_05s.wav   \n",
       "\n",
       "                                         earcon_filepath  \\\n",
       "0          dataset/earcon_dataset/earcons/BS_Bend_20.wav   \n",
       "1          dataset/earcon_dataset/earcons/Failure_01.wav   \n",
       "2          dataset/earcon_dataset/earcons/BS_Bend_17.wav   \n",
       "3               dataset/earcon_dataset/earcons/clock.wav   \n",
       "4          dataset/earcon_dataset/earcons/Failure_01.wav   \n",
       "...                                                  ...   \n",
       "11995  dataset/earcon_dataset/earcons/Testeregitar_-_...   \n",
       "11996  dataset/earcon_dataset/earcons/Anvil_-_Lokomo_...   \n",
       "11997  dataset/earcon_dataset/earcons/Short_Cut-Off_B...   \n",
       "11998      dataset/earcon_dataset/earcons/BS_Bend_17.wav   \n",
       "11999  dataset/earcon_dataset/earcons/01530_censore_b...   \n",
       "\n",
       "                                         earcon_features  \\\n",
       "0      [[tensor(930.), tensor(534.), tensor(530.), te...   \n",
       "1      [[tensor(913.), tensor(945.), tensor(530.), te...   \n",
       "2      [[tensor(319.), tensor(698.), tensor(857.), te...   \n",
       "3      [[tensor(62.), tensor(62.), tensor(62.), tenso...   \n",
       "4      [[tensor(913.), tensor(945.), tensor(530.), te...   \n",
       "...                                                  ...   \n",
       "11995  [[tensor(834.), tensor(604.), tensor(432.), te...   \n",
       "11996  [[tensor(170.), tensor(748.), tensor(748.), te...   \n",
       "11997  [[tensor(103.), tensor(56.), tensor(0.), tenso...   \n",
       "11998  [[tensor(319.), tensor(698.), tensor(857.), te...   \n",
       "11999  [[tensor(1009.), tensor(205.), tensor(606.), t...   \n",
       "\n",
       "                 image_filename  \\\n",
       "0           Coast-Test (1).jpeg   \n",
       "1         Coast-Test (100).jpeg   \n",
       "2         Coast-Test (101).jpeg   \n",
       "3         Coast-Test (102).jpeg   \n",
       "4         Coast-Test (103).jpeg   \n",
       "...                         ...   \n",
       "11995  Mountain-Valid (95).jpeg   \n",
       "11996  Mountain-Valid (96).jpeg   \n",
       "11997  Mountain-Valid (97).jpeg   \n",
       "11998  Mountain-Valid (98).jpeg   \n",
       "11999  Mountain-Valid (99).jpeg   \n",
       "\n",
       "                                          image_filepath  \\\n",
       "0      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "1      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "2      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "3      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "4      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "...                                                  ...   \n",
       "11995  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11996  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11997  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11998  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11999  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "\n",
       "                                          image_features  \\\n",
       "0      [tensor(0.5427), tensor(-0.2114), tensor(-0.54...   \n",
       "1      [tensor(0.1623), tensor(-0.0117), tensor(-0.17...   \n",
       "2      [tensor(0.5334), tensor(-0.1099), tensor(-0.36...   \n",
       "3      [tensor(0.8153), tensor(-0.2710), tensor(0.351...   \n",
       "4      [tensor(0.3290), tensor(-0.2365), tensor(-0.14...   \n",
       "...                                                  ...   \n",
       "11995  [tensor(-0.1323), tensor(0.4341), tensor(0.136...   \n",
       "11996  [tensor(-0.3210), tensor(0.1672), tensor(0.141...   \n",
       "11997  [tensor(0.3450), tensor(0.3041), tensor(0.0990...   \n",
       "11998  [tensor(0.1858), tensor(-0.0550), tensor(-0.14...   \n",
       "11999  [tensor(0.0373), tensor(-0.1407), tensor(-0.11...   \n",
       "\n",
       "                  image_tag  image_tag_similarity  similarity_score  \\\n",
       "0       a natural landscape              0.098083          0.059398   \n",
       "1       a natural landscape              0.120972          0.057352   \n",
       "2          a calm landscape              0.133911          0.072991   \n",
       "3        a bright landscape              0.113892          0.056997   \n",
       "4        a bright landscape              0.127563          0.066242   \n",
       "...                     ...                   ...               ...   \n",
       "11995   a intense landscape              0.072266          0.040474   \n",
       "11996   a complex landscape              0.069275          0.056088   \n",
       "11997    a narrow landscape              0.117004          0.043641   \n",
       "11998     a sharp landscape              0.099854          0.048247   \n",
       "11999  a positive landscape              0.071838          0.049314   \n",
       "\n",
       "            roundness pseudoword  weighted_angles  \n",
       "0      tensor(0.5484)   juxuluji        98.713105  \n",
       "1      tensor(0.5156)     juxulu        92.807444  \n",
       "2      tensor(0.5630)     geleje       101.336326  \n",
       "3      tensor(0.5332)     juxulu        95.974916  \n",
       "4      tensor(0.5516)     geleje        99.289937  \n",
       "...               ...        ...              ...  \n",
       "11995  tensor(0.5445)   juxuluji        98.015541  \n",
       "11996  tensor(0.5127)   juxuluja        92.280799  \n",
       "11997  tensor(0.5323)   juxuluji        95.818241  \n",
       "11998  tensor(0.5541)   gelejegi        99.729109  \n",
       "11999  tensor(0.5655)   gelejegi       101.785030  \n",
       "\n",
       "[12000 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earcon_image_dataset = pd.read_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset based on the \"split\" column\n",
    "train_df = earcon_image_dataset[earcon_image_dataset['split'] == 'train']\n",
    "train_df = train_df.drop(columns='split')\n",
    "val_df = earcon_image_dataset[earcon_image_dataset['split'] == 'validation']\n",
    "val_df = val_df.drop(columns='split')\n",
    "test_df = earcon_image_dataset[earcon_image_dataset['split'] == 'test']\n",
    "test_df = test_df.drop(columns='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 75, 50)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=0.01, random_state=42)\n",
    "val_df = val_df.sample(frac=0.05, random_state=42)\n",
    "test_df = test_df.sample(frac=0.1, random_state=42)\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "from utils.musicgen_utils import create_earcon_dataloaders\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "train_loader, val_loader, test_loader = create_earcon_dataloaders(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model pipeline is as follows:\n",
    "- The Earcon Encodec Vector is the target\n",
    "- The VA Vector and Bouba-Kiki Value will be inputs to the model\n",
    "- The model will output a set of vectors which will be fed to the MusicGen Decoder along with the Pseudoword\n",
    "- The output of MusicGen Decoder will be encoded by Encodec\n",
    "- The output of Encodec will be considered the final output, and loss will be calculated based on the difference between this output and the target Encodec vector from the Earcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "from utils.musicgen_model import *\n",
    "from utils.musicgen_utils import *\n",
    "from transformers import MusicgenDecoderConfig\n",
    "\n",
    "\n",
    "# model_options = {\n",
    "#     \"freeze_musicgen_text_encoder\": False,\n",
    "#     \"freeze_musicgen_decoder\": False,\n",
    "#     \"freeze_encodec\": True,\n",
    "#     \"num_projection_layers\": 2,\n",
    "#     \"fusion_hidden_dims\": [256]\n",
    "# }\n",
    "\n",
    "model = MusicgenForImageLM(\n",
    "    MusicgenDecoderConfig(\n",
    "        num_codebooks=1,\n",
    "        # hidden_size=2048\n",
    "    ),\n",
    "    freeze_encodec=True,\n",
    "    freeze_musicgen=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = MusicgenImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in image_processor.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# hyper parameters\n",
    "patience = 10\n",
    "epochs = 100\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_musicgen_model(\n",
    "    model=model,\n",
    "    image_processor=image_processor,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    epochs=epochs,\n",
    "    model_learning_rate=lr,\n",
    "    processor_learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    patience=patience,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_musicgen_image_model(model, filename=f\"MusicGenModel_0{version}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_musicgen_image_processor(image_processor, filename=f\"MusicGenImageProcessor_0{version}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from outputs/MusicGenModel_09.pt\n",
      "Processor loaded from outputs/MusicGenImageProcessor_09.pt\n"
     ]
    }
   ],
   "source": [
    "# Loading model\n",
    "from utils.musicgen_model import *\n",
    "\n",
    "model = load_musicgen_image_model(filename=f\"MusicGenModel_0{version}.pt\")\n",
    "image_processor = load_musicgen_image_processor(filename=f\"MusicGenImageProcessor_0{version}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for temp in test_loader:\n",
    "    batch = temp\n",
    "    break\n",
    "\n",
    "inputs = image_processor.forward(batch[\"image_features\"].to(device), batch[\"roundness\"].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128])\n",
      "torch.Size([5, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\transformers\\models\\musicgen\\modeling_musicgen.py:539: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 129])\n",
      "torch.Size([5, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (129) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m result\n",
      "File \u001b[1;32mc:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\OneDrive - Nanyang Technological University\\FYP\\FYP\\utils\\musicgen_model.py:439\u001b[0m, in \u001b[0;36mMusicgenForImageLM.generate\u001b[1;34m(self, input_ids, attention_mask, generation_config, logits_processor, stopping_criteria, synced_gpus, streamer, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m     output_ids \u001b[38;5;241m=\u001b[39m outputs\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# Apply the pattern mask to the final ids\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_delay_pattern_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelay_pattern_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;66;03m# Revert the pattern delay mask by filtering the pad token id\u001b[39;00m\n\u001b[0;32m    443\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m output_ids[output_ids \u001b[38;5;241m!=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39m_pad_token_tensor]\u001b[38;5;241m.\u001b[39mreshape(\n\u001b[0;32m    444\u001b[0m     batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_codebooks, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    445\u001b[0m )\n",
      "File \u001b[1;32mf:\\OneDrive - Nanyang Technological University\\FYP\\FYP\\utils\\musicgen_model.py:520\u001b[0m, in \u001b[0;36mMusicgenForImageLM.apply_delay_pattern_mask\u001b[1;34m(input_ids, decoder_pad_token_mask)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28mprint\u001b[39m(decoder_pad_token_mask\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 520\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_pad_token_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_pad_token_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m input_ids\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (129) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "result = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 10 unique random image/earcon pairs for testing\n",
    "elements = random.sample(range(0, len(test_df)), 10)\n",
    "elements.sort()\n",
    "elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Earcons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the audio for the selected pairs\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "audio_list = []\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "for i in range(len(elements)):\n",
    "    audio_list.append(\n",
    "        generate_earcon(\n",
    "            model,\n",
    "            test_df[\"image_features\"].iloc[elements[i]],\n",
    "            test_df[\"roundness\"].iloc[elements[i]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display, Image\n",
    "\n",
    "\n",
    "def display_image(image_path):\n",
    "    display(Image(filename=image_path))\n",
    "\n",
    "\n",
    "def play_audio(filepath):\n",
    "    display(Audio(filepath))\n",
    "\n",
    "\n",
    "def play_generated_audio(audio, sampling_rate=24000):\n",
    "    display(Audio(audio[0].cpu().numpy(), rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the images and play the audio for every pair\n",
    "for i in range(len(elements)):\n",
    "    # display image\n",
    "    print(f\"Image {i+1}:\")\n",
    "    display_image(test_df[\"image_filepath\"].iloc[elements[i]])\n",
    "    # play paired earcon\n",
    "    print(f\"Original Paired Earcon {i+1}:\")\n",
    "    play_audio(test_df[\"earcon_filepath\"].iloc[elements[i]])\n",
    "    # play generated earcon\n",
    "    print(f\"Generated Earcon {i+1}:\")\n",
    "    play_generated_audio(audio_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

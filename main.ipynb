{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "This model will take in a valence-arousal vector and a Bouba-Kiki vector. It will then output an encoded earcon representation, which will be passed to the MusicGen Decoder to generate the final earcon.\n",
    "\n",
    "The output of the MusicGen Decoder will then be encoded by EncodecFeatureExtractor, and the vectors will be used to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Each row in the dataset will consist of:\n",
    "- An Earcon represented by an Encodec vector\n",
    "- An image represented in a Valence Arousal Vector\n",
    "- A Bouba-Kiki Value derived from the image\n",
    "- A Pseudoword\n",
    "\n",
    "The rows will be paired by cosine similarity between the Earcon's Encodec vector and the VA Vector from the image. The Bouba-Kiki Value and Pseudoword will be generated after the images are paired with the audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in earcons\n",
    "earcons = pd.read_csv('dataset\\earcon_dataset\\earcon_dataset.csv')\n",
    "\n",
    "earcons['query'] = earcons['query'].apply(ast.literal_eval)\n",
    "earcons[\"query\"] = earcons[\"query\"].apply(lambda x: x[0])\n",
    "\n",
    "earcons = earcons[[\"query\", \"name\"]]\n",
    "\n",
    "earcons[\"filepaths\"] = earcons[\"name\"].apply(lambda x: f\"dataset/earcon_dataset/earcons/{x}\")\n",
    "\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep encodec model\n",
    "from encodec import EncodecModel\n",
    "\n",
    "\n",
    "encodec_model = EncodecModel.encodec_model_24khz().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "def extract_earcon_features(filepaths, encodec_model, target_sample_rate=24000, target_length=512):\n",
    "    earcon_features = []\n",
    "\n",
    "    for path in filepaths:\n",
    "        # load in the audio file\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        waveform = waveform.to(device)\n",
    "\n",
    "        # if stereo, convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True).to(device)\n",
    "\n",
    "        # resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate).to(device)\n",
    "            waveform = resampler(waveform).to(device)\n",
    "\n",
    "        # add batch dimension so that the shape is\n",
    "        # [1, 1, num_samples] because encodec\n",
    "        # expects that format\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        # encode the waveform\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = encodec_model.encode(waveform)\n",
    "            compressed_features = encoded_frames[0][0].to(device)  # Take the first codebook\n",
    "        \n",
    "        # truncate and pad\n",
    "        length = compressed_features.shape[2]\n",
    "        if length > target_length:\n",
    "            compressed_features = compressed_features[:, :, :target_length].to(device)\n",
    "        else:\n",
    "            pad = torch.zeros((compressed_features.shape[0], compressed_features.shape[1], target_length - length)).to(device)\n",
    "            compressed_features = torch.cat((compressed_features, pad), dim=2).to(device)\n",
    "\n",
    "        # remove the first dimension\n",
    "        compressed_features = compressed_features.squeeze(0)\n",
    "        earcon_features.append(compressed_features.to(\"cpu\"))\n",
    "\n",
    "    return earcon_features\n",
    "\n",
    "\n",
    "# Apply the function to all rows in the earcons dataframe\n",
    "earcons[\"earcon_features\"] = extract_earcon_features(earcons[\"filepaths\"], encodec_model)\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcons[\"earcon_features\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the largest value embedded in the earcon features\n",
    "\n",
    "smallest = 1000000\n",
    "largest = 0\n",
    "for i in range(len(earcons[\"earcon_features\"])):\n",
    "    if largest < int(earcons[\"earcon_features\"][i].max()):\n",
    "        largest = int(earcons[\"earcon_features\"][i].max())\n",
    "    if smallest > int(earcons[\"earcon_features\"][i].min()):\n",
    "        smallest = int(earcons[\"earcon_features\"][i].min())\n",
    "\n",
    "print(largest)\n",
    "print(smallest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in images\n",
    "images = pd.read_csv('dataset\\landscape1\\csvs\\image_classification.csv')\n",
    "\n",
    "# extract top tag and similarity score\n",
    "images['top_tags'] = images['top_tags'].apply(ast.literal_eval)\n",
    "images[\"top_tags\"] = images[\"top_tags\"].apply(lambda x: x[0])\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(ast.literal_eval)\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(lambda x: x[0])\n",
    "\n",
    "images[\"image_path\"] = images[\"image_path\"].str.lstrip(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load CLIP model\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Function to calculate image vectors\n",
    "def calculate_image_vectors(image_paths, clip_model, clip_processor):\n",
    "    image_features = []\n",
    "    count = 0\n",
    "    for image_path in image_paths:\n",
    "        count += 1\n",
    "        image = clip_processor(images=Image.open(image_path), return_tensors=\"pt\").pixel_values.to(device)\n",
    "        with torch.no_grad():\n",
    "            image = clip_model.get_image_features(image)\n",
    "            image = image.squeeze(0)\n",
    "            image_features.append(image.to(\"cpu\"))\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"Processed {count} images\")\n",
    "    return image_features\n",
    "\n",
    "\n",
    "# Apply the function to the images dataframe\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "images[\"image_features\"] = calculate_image_vectors(images[\"image_path\"].tolist(), clip_model, clip_processor)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[\"image_features\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity\n",
    "\n",
    "This will be used to build the dataset for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity & store in a new df\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_torch_cosine_similarity(image_vectors, earcon_vectors):\n",
    "    # Convert to PyTorch tensors\n",
    "    image_tensor = torch.tensor(np.stack(image_vectors))\n",
    "    earcon_tensor = torch.tensor(np.mean(np.stack(earcon_vectors), axis=1)).float()\n",
    "    \n",
    "    # Normalize vectors\n",
    "    image_tensor_norm = image_tensor / image_tensor.norm(dim=1, keepdim=True)\n",
    "    earcon_tensor_norm = earcon_tensor / earcon_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = torch.mm(image_tensor_norm, earcon_tensor_norm.t())\n",
    "    \n",
    "    return similarity_matrix.numpy()\n",
    "\n",
    "\n",
    "def process_similarities(images, earcons):\n",
    "    # Compute similarity matrix (using one of the methods above)\n",
    "    similarity_matrix = compute_torch_cosine_similarity(\n",
    "        images['image_features'].tolist(), \n",
    "        earcons['earcon_features'].tolist()\n",
    "    )\n",
    "    \n",
    "    # Find the index of the most similar earcon for each image\n",
    "    most_similar_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Extract the most similar earcon details\n",
    "    result_df = pd.DataFrame({\n",
    "        'split': images['split'],\n",
    "        'earcon_filename': earcons.iloc[most_similar_indices]['name'].values,\n",
    "        'earcon_filepath': earcons.iloc[most_similar_indices]['filepaths'].values,\n",
    "        'earcon_features': earcons.iloc[most_similar_indices]['earcon_features'].values,\n",
    "        'image_filename': images['filename'],\n",
    "        'image_filepath': images['image_path'],\n",
    "        'image_features': images['image_features'],\n",
    "        'image_tag': images['top_tags'],\n",
    "        'image_tag_similarity': images['similarity_scores'],\n",
    "        'similarity_score': similarity_matrix[np.arange(len(most_similar_indices)), most_similar_indices]\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "earcon_image_dataset = process_similarities(images, earcons)\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.earcon_filename.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pseudowords and Bouba-Kiki value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import utils.psword_gen as psword_gen\n",
    "import utils.psword_utils as psword_utils\n",
    "\n",
    "\n",
    "def generate_pseudoword_and_bouba_kiki(image_path, sound_dict):\n",
    "    x_values, y_values = psword_utils.process_image(image_path, 50, 150)\n",
    "    weighted_angles, roundness = psword_utils.calculate_weighted_angles_by_edge_length(x_values, y_values)\n",
    "    \n",
    "    random.seed(42)\n",
    "    \n",
    "    psword = psword_gen.pseudoword_generator(\n",
    "        roundness,\n",
    "        len(x_values),\n",
    "        sound_dict=sound_dict\n",
    "    )\n",
    "    \n",
    "    return roundness, psword, weighted_angles\n",
    "\n",
    "# Parallelized function\n",
    "def process_row(row):\n",
    "    return generate_pseudoword_and_bouba_kiki(row['image_filepath'], sound_dict)\n",
    "\n",
    "\n",
    "# sound dict\n",
    "sound_dict = psword_gen.load_sound_mappings('utils/sound_mappings.json')\n",
    "\n",
    "# Parallelize using joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(process_row)(row) for _, row in earcon_image_dataset.iterrows())\n",
    "\n",
    "# Extract and assign results\n",
    "earcon_image_dataset[['roundness', 'pseudoword', 'weighted_angles']] = pd.DataFrame(results, index=earcon_image_dataset.index)\n",
    "\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.to_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earcon_image_dataset = pd.read_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset based on the \"split\" column\n",
    "train_df = earcon_image_dataset[earcon_image_dataset['split'] == 'train']\n",
    "train_df = train_df.drop(columns='split')\n",
    "val_df = earcon_image_dataset[earcon_image_dataset['split'] == 'validation']\n",
    "val_df = val_df.drop(columns='split')\n",
    "test_df = earcon_image_dataset[earcon_image_dataset['split'] == 'test']\n",
    "test_df = test_df.drop(columns='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=0.01, random_state=42)\n",
    "val_df = val_df.sample(frac=0.05, random_state=42)\n",
    "test_df = test_df.sample(frac=1, random_state=42)\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from utils.musicgen_utils import create_earcon_dataloaders\n",
    "train_loader, val_loader, test_loader = create_earcon_dataloaders(train_df, val_df, test_df, batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model pipeline is as follows:\n",
    "- The Earcon Encodec Vector is the target\n",
    "- The VA Vector and Bouba-Kiki Value will be inputs to the model\n",
    "- The model will output a set of vectors which will be fed to the MusicGen Decoder along with the Pseudoword\n",
    "- The output of MusicGen Decoder will be encoded by Encodec\n",
    "- The output of Encodec will be considered the final output, and loss will be calculated based on the difference between this output and the target Encodec vector from the Earcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "c:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\torchaudio\\functional\\functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (257) may be set too low.\n",
      "  warnings.warn(\n",
      "c:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\transformers\\models\\encodec\\modeling_encodec.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "# init model\n",
    "from utils.musicgen_model import *\n",
    "from utils.musicgen_utils import *\n",
    "\n",
    "\n",
    "model = MultimodalEarconGenerator(\n",
    "    freeze_musicgen=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "trainable_params = [\n",
    "    param for param in model.parameters() if param.requires_grad\n",
    "]\n",
    "optimizer = optim.Adam(\n",
    "    trainable_params,\n",
    "    lr=0.001,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    "    weight_decay=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalEarconGenerator(\n",
      "  (encodec): EncodecModel(\n",
      "    (encoder): SEANetEncoder(\n",
      "      (model): Sequential(\n",
      "        (0): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(1, 32, kernel_size=(7,), stride=(1,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (1): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(32, 16, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): ELU(alpha=1.0)\n",
      "        (3): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(32, 64, kernel_size=(4,), stride=(2,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (4): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): ELU(alpha=1.0)\n",
      "        (6): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(64, 128, kernel_size=(8,), stride=(4,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (7): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): ELU(alpha=1.0)\n",
      "        (9): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(128, 256, kernel_size=(10,), stride=(5,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (10): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (11): ELU(alpha=1.0)\n",
      "        (12): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(256, 512, kernel_size=(16,), stride=(8,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (13): SLSTM(\n",
      "          (lstm): LSTM(512, 512, num_layers=2)\n",
      "        )\n",
      "        (14): ELU(alpha=1.0)\n",
      "        (15): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(512, 128, kernel_size=(7,), stride=(1,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (quantizer): ResidualVectorQuantizer(\n",
      "      (vq): ResidualVectorQuantization(\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x VectorQuantization(\n",
      "            (project_in): Identity()\n",
      "            (project_out): Identity()\n",
      "            (_codebook): EuclideanCodebook()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): SEANetDecoder(\n",
      "      (model): Sequential(\n",
      "        (0): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(128, 512, kernel_size=(7,), stride=(1,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (1): SLSTM(\n",
      "          (lstm): LSTM(512, 512, num_layers=2)\n",
      "        )\n",
      "        (2): ELU(alpha=1.0)\n",
      "        (3): SConvTranspose1d(\n",
      "          (convtr): NormConvTranspose1d(\n",
      "            (convtr): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (4): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(256, 256, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): ELU(alpha=1.0)\n",
      "        (6): SConvTranspose1d(\n",
      "          (convtr): NormConvTranspose1d(\n",
      "            (convtr): ConvTranspose1d(256, 128, kernel_size=(10,), stride=(5,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (7): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): ELU(alpha=1.0)\n",
      "        (9): SConvTranspose1d(\n",
      "          (convtr): NormConvTranspose1d(\n",
      "            (convtr): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (10): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (11): ELU(alpha=1.0)\n",
      "        (12): SConvTranspose1d(\n",
      "          (convtr): NormConvTranspose1d(\n",
      "            (convtr): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "        (13): SEANetResnetBlock(\n",
      "          (block): Sequential(\n",
      "            (0): ELU(alpha=1.0)\n",
      "            (1): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(32, 16, kernel_size=(3,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "            (2): ELU(alpha=1.0)\n",
      "            (3): SConv1d(\n",
      "              (conv): NormConv1d(\n",
      "                (conv): Conv1d(16, 32, kernel_size=(1,), stride=(1,))\n",
      "                (norm): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (shortcut): SConv1d(\n",
      "            (conv): NormConv1d(\n",
      "              (conv): Conv1d(32, 32, kernel_size=(1,), stride=(1,))\n",
      "              (norm): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (14): ELU(alpha=1.0)\n",
      "        (15): SConv1d(\n",
      "          (conv): NormConv1d(\n",
      "            (conv): Conv1d(32, 1, kernel_size=(7,), stride=(1,))\n",
      "            (norm): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (loss_fn): MultimodalEarconLoss(\n",
      "    (mel_spectrogram): Sequential(\n",
      "      (0): MelSpectrogram(\n",
      "        (spectrogram): Spectrogram()\n",
      "        (mel_scale): MelScale()\n",
      "      )\n",
      "      (1): LogTransform()\n",
      "    )\n",
      "  )\n",
      "  (musicgen): MusicgenForConditionalGeneration(\n",
      "    (text_encoder): T5EncoderModel(\n",
      "      (shared): Embedding(32128, 768)\n",
      "      (encoder): T5Stack(\n",
      "        (embed_tokens): Embedding(32128, 768)\n",
      "        (block): ModuleList(\n",
      "          (0): T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (relative_attention_bias): Embedding(32, 12)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-11): 11 x T5Block(\n",
      "            (layer): ModuleList(\n",
      "              (0): T5LayerSelfAttention(\n",
      "                (SelfAttention): T5Attention(\n",
      "                  (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (1): T5LayerFF(\n",
      "                (DenseReluDense): T5DenseActDense(\n",
      "                  (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "                  (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                  (act): ReLU()\n",
      "                )\n",
      "                (layer_norm): T5LayerNorm()\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): T5LayerNorm()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (audio_encoder): EncodecModel(\n",
      "      (encoder): EncodecEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): EncodecConv1d(\n",
      "            (conv): Conv1d(1, 64, kernel_size=(7,), stride=(1,))\n",
      "          )\n",
      "          (1): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (2): ELU(alpha=1.0)\n",
      "          (3): EncodecConv1d(\n",
      "            (conv): Conv1d(64, 128, kernel_size=(8,), stride=(4,))\n",
      "          )\n",
      "          (4): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (5): ELU(alpha=1.0)\n",
      "          (6): EncodecConv1d(\n",
      "            (conv): Conv1d(128, 256, kernel_size=(8,), stride=(4,))\n",
      "          )\n",
      "          (7): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (8): ELU(alpha=1.0)\n",
      "          (9): EncodecConv1d(\n",
      "            (conv): Conv1d(256, 512, kernel_size=(10,), stride=(5,))\n",
      "          )\n",
      "          (10): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (11): ELU(alpha=1.0)\n",
      "          (12): EncodecConv1d(\n",
      "            (conv): Conv1d(512, 1024, kernel_size=(16,), stride=(8,))\n",
      "          )\n",
      "          (13): EncodecLSTM(\n",
      "            (lstm): LSTM(1024, 1024, num_layers=2)\n",
      "          )\n",
      "          (14): ELU(alpha=1.0)\n",
      "          (15): EncodecConv1d(\n",
      "            (conv): Conv1d(1024, 128, kernel_size=(7,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (decoder): EncodecDecoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): EncodecConv1d(\n",
      "            (conv): Conv1d(128, 1024, kernel_size=(7,), stride=(1,))\n",
      "          )\n",
      "          (1): EncodecLSTM(\n",
      "            (lstm): LSTM(1024, 1024, num_layers=2)\n",
      "          )\n",
      "          (2): ELU(alpha=1.0)\n",
      "          (3): EncodecConvTranspose1d(\n",
      "            (conv): ConvTranspose1d(1024, 512, kernel_size=(16,), stride=(8,))\n",
      "          )\n",
      "          (4): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(512, 256, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (5): ELU(alpha=1.0)\n",
      "          (6): EncodecConvTranspose1d(\n",
      "            (conv): ConvTranspose1d(512, 256, kernel_size=(10,), stride=(5,))\n",
      "          )\n",
      "          (7): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(256, 128, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (8): ELU(alpha=1.0)\n",
      "          (9): EncodecConvTranspose1d(\n",
      "            (conv): ConvTranspose1d(256, 128, kernel_size=(8,), stride=(4,))\n",
      "          )\n",
      "          (10): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(128, 64, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (11): ELU(alpha=1.0)\n",
      "          (12): EncodecConvTranspose1d(\n",
      "            (conv): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(4,))\n",
      "          )\n",
      "          (13): EncodecResnetBlock(\n",
      "            (block): ModuleList(\n",
      "              (0): ELU(alpha=1.0)\n",
      "              (1): EncodecConv1d(\n",
      "                (conv): Conv1d(64, 32, kernel_size=(3,), stride=(1,))\n",
      "              )\n",
      "              (2): ELU(alpha=1.0)\n",
      "              (3): EncodecConv1d(\n",
      "                (conv): Conv1d(32, 64, kernel_size=(1,), stride=(1,))\n",
      "              )\n",
      "            )\n",
      "            (shortcut): Identity()\n",
      "          )\n",
      "          (14): ELU(alpha=1.0)\n",
      "          (15): EncodecConv1d(\n",
      "            (conv): Conv1d(64, 1, kernel_size=(7,), stride=(1,))\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (quantizer): EncodecResidualVectorQuantizer(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x EncodecVectorQuantization(\n",
      "            (codebook): EncodecEuclideanCodebook()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): MusicgenForCausalLM(\n",
      "      (model): MusicgenModel(\n",
      "        (decoder): MusicgenDecoder(\n",
      "          (embed_tokens): ModuleList(\n",
      "            (0-3): 4 x Embedding(2049, 1024)\n",
      "          )\n",
      "          (embed_positions): MusicgenSinusoidalPositionalEmbedding()\n",
      "          (layers): ModuleList(\n",
      "            (0-23): 24 x MusicgenDecoderLayer(\n",
      "              (self_attn): MusicgenSdpaAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              )\n",
      "              (activation_fn): GELUActivation()\n",
      "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (encoder_attn): MusicgenSdpaAttention(\n",
      "                (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (q_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "                (out_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "              )\n",
      "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (lm_heads): ModuleList(\n",
      "        (0-3): 4 x Linear(in_features=1024, out_features=2048, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (enc_to_dec_proj): Linear(in_features=768, out_features=1024, bias=True)\n",
      "  )\n",
      "  (image_projection): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (roundness_projection): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (modality_fusion): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_multimodal_earcon_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_multimodal_model(model, model.loss_fn, prefix=\"MusicGenModel_04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "\n",
    "model = MultimodalEarconGenerator()\n",
    "model = load_multimodal_model(model, model_path=\"outputs\\MusicGenModel_04_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# choose 10 unique random image/earcon pairs for testing\n",
    "elements = random.sample(range(0, len(test_df)), 10)\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the audio for the selected pairs\n",
    "\n",
    "audio_list = []\n",
    "\n",
    "for i in range(len(elements)):\n",
    "    audio_list.append(\n",
    "        generate_earcon(\n",
    "            model,\n",
    "            test_df[\"image_features\"].iloc[elements[i]],\n",
    "            test_df[\"roundness\"].iloc[elements[i]],\n",
    "            image_tag=test_df[\"image_tag\"].iloc[elements[i]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display, Image\n",
    "\n",
    "\n",
    "def display_image(image_path):\n",
    "    display(Image(filename=image_path))\n",
    "\n",
    "\n",
    "def play_audio(filepath):\n",
    "    display(Audio(filepath))\n",
    "\n",
    "\n",
    "def play_generated_audio(audio, sampling_rate=24000):\n",
    "    display(Audio(audio[0].cpu().numpy(), rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the images and play the audio for every pair\n",
    "for i in range(len(elements)):\n",
    "    # display image\n",
    "    print(f\"Image {i+1}:\")\n",
    "    display_image(test_df[\"image_filepath\"].iloc[elements[i]])\n",
    "    # play paired earcon\n",
    "    print(f\"Original Paired Earcon {i+1}:\")\n",
    "    play_audio(test_df[\"earcon_filepath\"].iloc[elements[i]])\n",
    "    # play generated earcon\n",
    "    print(f\"Generated Earcon {i+1}:\")\n",
    "    play_generated_audio(audio_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

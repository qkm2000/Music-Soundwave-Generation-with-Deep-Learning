{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "This model will take in a valence-arousal vector and a Bouba-Kiki vector. It will then output an encoded earcon representation, which will be passed to the MusicGen Decoder to generate the final earcon.\n",
    "\n",
    "The output of the MusicGen Decoder will then be encoded by EncodecFeatureExtractor, and the vectors will be used to calculate the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataset will consist of:\n",
    "- An Earcon represented by an Encodec vector\n",
    "- An image represented in a Valence Arousal Vector\n",
    "- A Bouba-Kiki Value derived from the image\n",
    "- A Pseudoword\n",
    "\n",
    "The rows will be paired by cosine similarity between the Earcon's Encodec vector and the VA Vector from the image. The Bouba-Kiki Value and Pseudoword will be generated after the images are paired with the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in earcons\n",
    "earcons = pd.read_csv('dataset\\earcon_dataset\\earcon_dataset.csv')\n",
    "\n",
    "earcons['query'] = earcons['query'].apply(ast.literal_eval)\n",
    "earcons[\"query\"] = earcons[\"query\"].apply(lambda x: x[0])\n",
    "\n",
    "earcons = earcons[[\"query\", \"name\"]]\n",
    "\n",
    "earcons[\"filepaths\"] = earcons[\"name\"].apply(lambda x: f\"dataset/earcon_dataset/earcons/{x}\")\n",
    "\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep encodec model\n",
    "from encodec import EncodecModel\n",
    "\n",
    "\n",
    "encodec_model = EncodecModel.encodec_model_24khz().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "def extract_earcon_features(filepaths, encodec_model, target_sample_rate=24000, target_length=512):\n",
    "    earcon_features = []\n",
    "\n",
    "    for path in filepaths:\n",
    "        # load in the audio file\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        waveform = waveform.to(device)\n",
    "\n",
    "        # if stereo, convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True).to(device)\n",
    "\n",
    "        # resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate).to(device)\n",
    "            waveform = resampler(waveform).to(device)\n",
    "\n",
    "        # add batch dimension so that the shape is\n",
    "        # [1, 1, num_samples] because encodec\n",
    "        # expects that format\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        # encode the waveform\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = encodec_model.encode(waveform)\n",
    "            compressed_features = encoded_frames[0][0].to(device)  # Take the first codebook\n",
    "        \n",
    "        # truncate and pad\n",
    "        length = compressed_features.shape[2]\n",
    "        if length > target_length:\n",
    "            compressed_features = compressed_features[:, :, :target_length].to(device)\n",
    "        else:\n",
    "            pad = torch.zeros((compressed_features.shape[0], compressed_features.shape[1], target_length - length)).to(device)\n",
    "            compressed_features = torch.cat((compressed_features, pad), dim=2).to(device)\n",
    "\n",
    "        # remove the first dimension\n",
    "        compressed_features = compressed_features.squeeze(0)\n",
    "        earcon_features.append(compressed_features.to(\"cpu\"))\n",
    "\n",
    "    return earcon_features\n",
    "\n",
    "\n",
    "# Apply the function to all rows in the earcons dataframe\n",
    "earcons[\"earcon_features\"] = extract_earcon_features(earcons[\"filepaths\"], encodec_model)\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcons[\"earcon_features\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the largest value embedded in the earcon features\n",
    "\n",
    "smallest = 1000000\n",
    "largest = 0\n",
    "for i in range(len(earcons[\"earcon_features\"])):\n",
    "    if largest < int(earcons[\"earcon_features\"][i].max()):\n",
    "        largest = int(earcons[\"earcon_features\"][i].max())\n",
    "    if smallest > int(earcons[\"earcon_features\"][i].min()):\n",
    "        smallest = int(earcons[\"earcon_features\"][i].min())\n",
    "\n",
    "print(largest)\n",
    "print(smallest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in images\n",
    "images = pd.read_csv('dataset\\landscape1\\csvs\\image_classification.csv')\n",
    "\n",
    "# extract top tag and similarity score\n",
    "images['top_tags'] = images['top_tags'].apply(ast.literal_eval)\n",
    "images[\"top_tags\"] = images[\"top_tags\"].apply(lambda x: x[0])\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(ast.literal_eval)\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(lambda x: x[0])\n",
    "\n",
    "images[\"image_path\"] = images[\"image_path\"].str.lstrip(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load CLIP model\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Function to calculate image vectors\n",
    "def calculate_image_vectors(image_paths, clip_model, clip_processor):\n",
    "    image_features = []\n",
    "    count = 0\n",
    "    for image_path in image_paths:\n",
    "        count += 1\n",
    "        image = clip_processor(images=Image.open(image_path), return_tensors=\"pt\").pixel_values.to(device)\n",
    "        with torch.no_grad():\n",
    "            image = clip_model.get_image_features(image)\n",
    "            print(image)\n",
    "            image = image.squeeze(0)\n",
    "            image_features.append(image.to(\"cpu\"))\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"Processed {count} images\")\n",
    "    return image_features\n",
    "\n",
    "\n",
    "# Apply the function to the images dataframe\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "images[\"image_features\"] = calculate_image_vectors(images[\"image_path\"].tolist(), clip_model, clip_processor)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[\"image_features\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity\n",
    "\n",
    "This will be used to build the dataset for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity & store in a new df\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_torch_cosine_similarity(image_vectors, earcon_vectors):\n",
    "    # Convert to PyTorch tensors\n",
    "    image_tensor = torch.tensor(np.stack(image_vectors))\n",
    "    earcon_tensor = torch.tensor(np.mean(np.stack(earcon_vectors), axis=1)).float()\n",
    "    \n",
    "    # Normalize vectors\n",
    "    image_tensor_norm = image_tensor / image_tensor.norm(dim=1, keepdim=True)\n",
    "    earcon_tensor_norm = earcon_tensor / earcon_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = torch.mm(image_tensor_norm, earcon_tensor_norm.t())\n",
    "    \n",
    "    return similarity_matrix.numpy()\n",
    "\n",
    "\n",
    "def process_similarities(images, earcons):\n",
    "    # Compute similarity matrix (using one of the methods above)\n",
    "    similarity_matrix = compute_torch_cosine_similarity(\n",
    "        images['image_features'].tolist(), \n",
    "        earcons['earcon_features'].tolist()\n",
    "    )\n",
    "    \n",
    "    # Find the index of the most similar earcon for each image\n",
    "    most_similar_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Extract the most similar earcon details\n",
    "    result_df = pd.DataFrame({\n",
    "        'split': images['split'],\n",
    "        'earcon_filename': earcons.iloc[most_similar_indices]['name'].values,\n",
    "        'earcon_filepath': earcons.iloc[most_similar_indices]['filepaths'].values,\n",
    "        'earcon_features': earcons.iloc[most_similar_indices]['earcon_features'].values,\n",
    "        'image_filename': images['filename'],\n",
    "        'image_filepath': images['image_path'],\n",
    "        'image_features': images['image_features'],\n",
    "        'image_tag': images['top_tags'],\n",
    "        'image_tag_similarity': images['similarity_scores'],\n",
    "        'similarity_score': similarity_matrix[np.arange(len(most_similar_indices)), most_similar_indices]\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "earcon_image_dataset = process_similarities(images, earcons)\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.earcon_filename.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pseudowords and Bouba-Kiki value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import utils.psword_gen as psword_gen\n",
    "import utils.psword_utils as psword_utils\n",
    "\n",
    "\n",
    "def generate_pseudoword_and_bouba_kiki(image_path, sound_dict):\n",
    "    x_values, y_values = psword_utils.process_image(image_path, 50, 150)\n",
    "    weighted_angles, roundness = psword_utils.calculate_weighted_angles_by_edge_length(x_values, y_values)\n",
    "    \n",
    "    random.seed(42)\n",
    "    \n",
    "    psword = psword_gen.pseudoword_generator(\n",
    "        roundness,\n",
    "        len(x_values),\n",
    "        sound_dict=sound_dict\n",
    "    )\n",
    "\n",
    "    roundness = torch.tensor(roundness).float()\n",
    "    \n",
    "    return roundness, psword, weighted_angles\n",
    "\n",
    "# Parallelized function\n",
    "def process_row(row):\n",
    "    return generate_pseudoword_and_bouba_kiki(row['image_filepath'], sound_dict)\n",
    "\n",
    "\n",
    "# sound dict\n",
    "sound_dict = psword_gen.load_sound_mappings('utils/sound_mappings.json')\n",
    "\n",
    "# Parallelize using joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(process_row)(row) for _, row in earcon_image_dataset.iterrows())\n",
    "\n",
    "# Extract and assign results\n",
    "earcon_image_dataset[['roundness', 'pseudoword', 'weighted_angles']] = pd.DataFrame(results, index=earcon_image_dataset.index)\n",
    "\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.to_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset = pd.read_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset based on the \"split\" column\n",
    "train_df = earcon_image_dataset[earcon_image_dataset['split'] == 'train']\n",
    "train_df = train_df.drop(columns='split')\n",
    "val_df = earcon_image_dataset[earcon_image_dataset['split'] == 'validation']\n",
    "val_df = val_df.drop(columns='split')\n",
    "test_df = earcon_image_dataset[earcon_image_dataset['split'] == 'test']\n",
    "test_df = test_df.drop(columns='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=0.05, random_state=42)\n",
    "val_df = val_df.sample(frac=0.1, random_state=42)\n",
    "test_df = test_df.sample(frac=1, random_state=42)\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from utils.musicgen_utils import create_earcon_dataloaders\n",
    "\n",
    "\n",
    "batch_size = 10\n",
    "train_loader, val_loader, test_loader = create_earcon_dataloaders(\n",
    "    train_df,\n",
    "    val_df,\n",
    "    test_df,\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model pipeline is as follows:\n",
    "- The Earcon Encodec Vector is the target\n",
    "- The VA Vector and Bouba-Kiki Value will be inputs to the model\n",
    "- The model will output a set of vectors which will be fed to the MusicGen Decoder along with the Pseudoword\n",
    "- The output of MusicGen Decoder will be encoded by Encodec\n",
    "- The output of Encodec will be considered the final output, and loss will be calculated based on the difference between this output and the target Encodec vector from the Earcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "from utils.musicgen_model import *\n",
    "from utils.musicgen_utils import *\n",
    "from transformers import MusicgenDecoderConfig\n",
    "\n",
    "\n",
    "model_options = {\n",
    "    \"freeze_musicgen_text_encoder\": False,\n",
    "    \"freeze_musicgen_decoder\": False,\n",
    "    \"freeze_encodec\": True,\n",
    "    \"num_projection_layers\": 2,\n",
    "    \"fusion_hidden_dims\": [256]\n",
    "}\n",
    "\n",
    "model = MusicgenForImageLM(\n",
    "    MusicgenDecoderConfig(\n",
    "        num_codebooks=1,\n",
    "        # hidden_size=2048\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is None:\n",
    "        print(f\"No gradient for {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# hyper parameters\n",
    "patience = 3\n",
    "epochs = 10\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train model\n",
    "train_musicgen_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    test_dataloader=test_loader,\n",
    "    epochs=epochs,\n",
    "    learning_rate=lr,\n",
    "    patience=patience,\n",
    "    accumulation_steps=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_multimodal_model(model, filename=f\"MusicGenModel_0{version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "from utils.musicgen_model import *\n",
    "\n",
    "model = MultimodalEarconGenerator(\n",
    "    freeze_musicgen_text_encoder=model_options[\"freeze_musicgen_text_encoder\"],\n",
    "    freeze_musicgen_decoder=model_options[\"freeze_musicgen_decoder\"],\n",
    "    freeze_encodec=model_options[\"freeze_encodec\"],\n",
    "    num_projection_layers=model_options[\"num_projection_layers\"],\n",
    "    fusion_hidden_dims=model_options[\"fusion_hidden_dims\"])\n",
    "model = load_multimodal_model(model, model_path=f\"outputs/MusicGenModel_0{version}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose 10 unique random image/earcon pairs for testing\n",
    "elements = random.sample(range(0, len(test_df)), 10)\n",
    "elements.sort()\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the audio for the selected pairs\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "\n",
    "audio_list = []\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"facebook/musicgen-small\")\n",
    "\n",
    "for i in range(len(elements)):\n",
    "    audio_list.append(\n",
    "        generate_earcon(\n",
    "            model,\n",
    "            test_df[\"image_features\"].iloc[elements[i]],\n",
    "            test_df[\"roundness\"].iloc[elements[i]],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display, Image\n",
    "\n",
    "\n",
    "def display_image(image_path):\n",
    "    display(Image(filename=image_path))\n",
    "\n",
    "\n",
    "def play_audio(filepath):\n",
    "    display(Audio(filepath))\n",
    "\n",
    "\n",
    "def play_generated_audio(audio, sampling_rate=24000):\n",
    "    display(Audio(audio[0].cpu().numpy(), rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the images and play the audio for every pair\n",
    "for i in range(len(elements)):\n",
    "    # display image\n",
    "    print(f\"Image {i+1}:\")\n",
    "    display_image(test_df[\"image_filepath\"].iloc[elements[i]])\n",
    "    # play paired earcon\n",
    "    print(f\"Original Paired Earcon {i+1}:\")\n",
    "    play_audio(test_df[\"earcon_filepath\"].iloc[elements[i]])\n",
    "    # play generated earcon\n",
    "    print(f\"Generated Earcon {i+1}:\")\n",
    "    play_generated_audio(audio_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

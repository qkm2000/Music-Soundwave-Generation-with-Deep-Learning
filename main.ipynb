{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "This model will take in a valence-arousal vector and a Bouba-Kiki vector. It will then output an encoded earcon representation, which will be passed to the MusicGen Decoder to generate the final earcon.\n",
    "\n",
    "The output of the MusicGen Decoder will then be encoded by EncodecFeatureExtractor, and the vectors will be used to calculate the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Each row in the dataset will consist of:\n",
    "- An Earcon represented by an Encodec vector\n",
    "- An image represented in a Valence Arousal Vector\n",
    "- A Bouba-Kiki Value derived from the image\n",
    "- A Pseudoword\n",
    "\n",
    "The rows will be paired by cosine similarity between the Earcon's Encodec vector and the VA Vector from the image. The Bouba-Kiki Value and Pseudoword will be generated after the images are paired with the audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in earcons\n",
    "earcons = pd.read_csv('dataset\\earcon_dataset\\earcon_dataset.csv')\n",
    "\n",
    "earcons['query'] = earcons['query'].apply(ast.literal_eval)\n",
    "earcons[\"query\"] = earcons[\"query\"].apply(lambda x: x[0])\n",
    "\n",
    "earcons = earcons[[\"query\", \"name\"]]\n",
    "\n",
    "earcons[\"filepaths\"] = earcons[\"name\"].apply(lambda x: f\"dataset/earcon_dataset/earcons/{x}\")\n",
    "\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep encodec model\n",
    "from encodec import EncodecModel\n",
    "\n",
    "\n",
    "encodec_model = EncodecModel.encodec_model_24khz().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "\n",
    "def extract_earcon_features(filepaths, encodec_model, target_sample_rate=24000, target_length=512):\n",
    "    earcon_features = []\n",
    "\n",
    "    for path in filepaths:\n",
    "        # load in the audio file\n",
    "        waveform, sample_rate = torchaudio.load(path)\n",
    "        waveform = waveform.to(device)\n",
    "\n",
    "        # if stereo, convert to mono\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True).to(device)\n",
    "\n",
    "        # resample if necessary\n",
    "        if sample_rate != target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sample_rate, target_sample_rate).to(device)\n",
    "            waveform = resampler(waveform).to(device)\n",
    "\n",
    "        # add batch dimension so that the shape is\n",
    "        # [1, 1, num_samples] because encodec\n",
    "        # expects that format\n",
    "        waveform = waveform.unsqueeze(0)\n",
    "\n",
    "        # encode the waveform\n",
    "        with torch.no_grad():\n",
    "            encoded_frames = encodec_model.encode(waveform)\n",
    "            compressed_features = encoded_frames[0][0].to(device)  # Take the first codebook\n",
    "        \n",
    "        # truncate and pad\n",
    "        length = compressed_features.shape[2]\n",
    "        if length > target_length:\n",
    "            compressed_features = compressed_features[:, :, :target_length].to(device)\n",
    "        else:\n",
    "            pad = torch.zeros((compressed_features.shape[0], compressed_features.shape[1], target_length - length)).to(device)\n",
    "            compressed_features = torch.cat((compressed_features, pad), dim=2).to(device)\n",
    "\n",
    "        # remove the first dimension\n",
    "        compressed_features = compressed_features.squeeze(0)\n",
    "        earcon_features.append(compressed_features.to(\"cpu\"))\n",
    "\n",
    "    return earcon_features\n",
    "\n",
    "\n",
    "# Apply the function to all rows in the earcons dataframe\n",
    "earcons[\"earcon_features\"] = extract_earcon_features(earcons[\"filepaths\"], encodec_model)\n",
    "earcons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcons[\"earcon_features\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the largest value embedded in the earcon features\n",
    "\n",
    "smallest = 1000000\n",
    "largest = 0\n",
    "for i in range(len(earcons[\"earcon_features\"])):\n",
    "    if largest < int(earcons[\"earcon_features\"][i].max()):\n",
    "        largest = int(earcons[\"earcon_features\"][i].max())\n",
    "    if smallest > int(earcons[\"earcon_features\"][i].min()):\n",
    "        smallest = int(earcons[\"earcon_features\"][i].min())\n",
    "\n",
    "print(largest)\n",
    "print(smallest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in images\n",
    "images = pd.read_csv('dataset\\landscape1\\csvs\\image_classification.csv')\n",
    "\n",
    "# extract top tag and similarity score\n",
    "images['top_tags'] = images['top_tags'].apply(ast.literal_eval)\n",
    "images[\"top_tags\"] = images[\"top_tags\"].apply(lambda x: x[0])\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(ast.literal_eval)\n",
    "images[\"similarity_scores\"] = images[\"similarity_scores\"].apply(lambda x: x[0])\n",
    "\n",
    "images[\"image_path\"] = images[\"image_path\"].str.lstrip(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load CLIP model\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Function to calculate image vectors\n",
    "def calculate_image_vectors(image_paths, clip_model, clip_processor):\n",
    "    image_features = []\n",
    "    count = 0\n",
    "    for image_path in image_paths:\n",
    "        count += 1\n",
    "        image = clip_processor(images=Image.open(image_path), return_tensors=\"pt\").pixel_values.to(device)\n",
    "        with torch.no_grad():\n",
    "            image = clip_model.get_image_features(image)\n",
    "            image = image.squeeze(0)\n",
    "            image_features.append(image.to(\"cpu\"))\n",
    "        if count % 1000 == 0:\n",
    "            print(f\"Processed {count} images\")\n",
    "    return image_features\n",
    "\n",
    "\n",
    "# Apply the function to the images dataframe\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "images[\"image_features\"] = calculate_image_vectors(images[\"image_path\"].tolist(), clip_model, clip_processor)\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[\"image_features\"][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Cosine Similarity\n",
    "\n",
    "This will be used to build the dataset for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the cosine similarity & store in a new df\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def compute_torch_cosine_similarity(image_vectors, earcon_vectors):\n",
    "    # Convert to PyTorch tensors\n",
    "    image_tensor = torch.tensor(np.stack(image_vectors))\n",
    "    earcon_tensor = torch.tensor(np.mean(np.stack(earcon_vectors), axis=1)).float()\n",
    "    \n",
    "    # Normalize vectors\n",
    "    image_tensor_norm = image_tensor / image_tensor.norm(dim=1, keepdim=True)\n",
    "    earcon_tensor_norm = earcon_tensor / earcon_tensor.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity_matrix = torch.mm(image_tensor_norm, earcon_tensor_norm.t())\n",
    "    \n",
    "    return similarity_matrix.numpy()\n",
    "\n",
    "\n",
    "def process_similarities(images, earcons):\n",
    "    # Compute similarity matrix (using one of the methods above)\n",
    "    similarity_matrix = compute_torch_cosine_similarity(\n",
    "        images['image_features'].tolist(), \n",
    "        earcons['earcon_features'].tolist()\n",
    "    )\n",
    "    \n",
    "    # Find the index of the most similar earcon for each image\n",
    "    most_similar_indices = np.argmax(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Extract the most similar earcon details\n",
    "    result_df = pd.DataFrame({\n",
    "        'split': images['split'],\n",
    "        'earcon_filename': earcons.iloc[most_similar_indices]['name'].values,\n",
    "        'earcon_filepath': earcons.iloc[most_similar_indices]['filepaths'].values,\n",
    "        'earcon_features': earcons.iloc[most_similar_indices]['earcon_features'].values,\n",
    "        'image_filename': images['filename'],\n",
    "        'image_filepath': images['image_path'],\n",
    "        'image_features': images['image_features'],\n",
    "        'image_tag': images['top_tags'],\n",
    "        'image_tag_similarity': images['similarity_scores'],\n",
    "        'similarity_score': similarity_matrix[np.arange(len(most_similar_indices)), most_similar_indices]\n",
    "    })\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "earcon_image_dataset = process_similarities(images, earcons)\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.earcon_filename.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Pseudowords and Bouba-Kiki value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import utils.psword_gen as psword_gen\n",
    "import utils.psword_utils as psword_utils\n",
    "\n",
    "\n",
    "def generate_pseudoword_and_bouba_kiki(image_path, sound_dict):\n",
    "    x_values, y_values = psword_utils.process_image(image_path, 50, 150)\n",
    "    weighted_angles, roundness = psword_utils.calculate_weighted_angles_by_edge_length(x_values, y_values)\n",
    "    \n",
    "    random.seed(42)\n",
    "    \n",
    "    psword = psword_gen.pseudoword_generator(\n",
    "        roundness,\n",
    "        len(x_values),\n",
    "        sound_dict=sound_dict\n",
    "    )\n",
    "    \n",
    "    return roundness, psword, weighted_angles\n",
    "\n",
    "# Parallelized function\n",
    "def process_row(row):\n",
    "    return generate_pseudoword_and_bouba_kiki(row['image_filepath'], sound_dict)\n",
    "\n",
    "\n",
    "# sound dict\n",
    "sound_dict = psword_gen.load_sound_mappings('utils/sound_mappings.json')\n",
    "\n",
    "# Parallelize using joblib\n",
    "results = Parallel(n_jobs=-1)(delayed(process_row)(row) for _, row in earcon_image_dataset.iterrows())\n",
    "\n",
    "# Extract and assign results\n",
    "earcon_image_dataset[['roundness', 'pseudoword', 'weighted_angles']] = pd.DataFrame(results, index=earcon_image_dataset.index)\n",
    "\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset.to_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_image_dataset[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model pipeline is as follows:\n",
    "- The Earcon Encodec Vector is the target\n",
    "- The VA Vector and Bouba-Kiki Value will be inputs to the model\n",
    "- The model will output a set of vectors which will be fed to the MusicGen Decoder along with the Pseudoword\n",
    "- The output of MusicGen Decoder will be encoded by Encodec\n",
    "- The output of Encodec will be considered the final output, and loss will be calculated based on the difference between this output and the target Encodec vector from the Earcon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init model\n",
    "from utils.musicgen_model import *\n",
    "from utils.musicgen_utils import *\n",
    "\n",
    "\n",
    "model = CustomMusicGenModel(\n",
    "    freeze_musicgen=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>earcon_filename</th>\n",
       "      <th>earcon_filepath</th>\n",
       "      <th>earcon_features</th>\n",
       "      <th>image_filename</th>\n",
       "      <th>image_filepath</th>\n",
       "      <th>image_features</th>\n",
       "      <th>image_tag</th>\n",
       "      <th>image_tag_similarity</th>\n",
       "      <th>similarity_score</th>\n",
       "      <th>roundness</th>\n",
       "      <th>pseudoword</th>\n",
       "      <th>weighted_angles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test</td>\n",
       "      <td>BS_Bend_20.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/BS_Bend_20.wav</td>\n",
       "      <td>[[tensor(930.), tensor(534.), tensor(530.), te...</td>\n",
       "      <td>Coast-Test (1).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.5427), tensor(-0.2114), tensor(-0.54...</td>\n",
       "      <td>a natural landscape</td>\n",
       "      <td>0.098083</td>\n",
       "      <td>0.059398</td>\n",
       "      <td>0.548406</td>\n",
       "      <td>juxuluji</td>\n",
       "      <td>98.713105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>Failure_01.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Failure_01.wav</td>\n",
       "      <td>[[tensor(913.), tensor(945.), tensor(530.), te...</td>\n",
       "      <td>Coast-Test (100).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.1623), tensor(-0.0117), tensor(-0.17...</td>\n",
       "      <td>a natural landscape</td>\n",
       "      <td>0.120972</td>\n",
       "      <td>0.057352</td>\n",
       "      <td>0.515597</td>\n",
       "      <td>juxulu</td>\n",
       "      <td>92.807444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>BS_Bend_17.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/BS_Bend_17.wav</td>\n",
       "      <td>[[tensor(319.), tensor(698.), tensor(857.), te...</td>\n",
       "      <td>Coast-Test (101).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.5334), tensor(-0.1099), tensor(-0.36...</td>\n",
       "      <td>a calm landscape</td>\n",
       "      <td>0.133911</td>\n",
       "      <td>0.072991</td>\n",
       "      <td>0.562980</td>\n",
       "      <td>geleje</td>\n",
       "      <td>101.336326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test</td>\n",
       "      <td>clock.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/clock.wav</td>\n",
       "      <td>[[tensor(62.), tensor(62.), tensor(62.), tenso...</td>\n",
       "      <td>Coast-Test (102).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.8153), tensor(-0.2710), tensor(0.351...</td>\n",
       "      <td>a bright landscape</td>\n",
       "      <td>0.113892</td>\n",
       "      <td>0.056997</td>\n",
       "      <td>0.533194</td>\n",
       "      <td>juxulu</td>\n",
       "      <td>95.974916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test</td>\n",
       "      <td>Failure_01.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Failure_01.wav</td>\n",
       "      <td>[[tensor(913.), tensor(945.), tensor(530.), te...</td>\n",
       "      <td>Coast-Test (103).jpeg</td>\n",
       "      <td>dataset/landscape1/Testing Data/Coast\\Coast-Te...</td>\n",
       "      <td>[tensor(0.3290), tensor(-0.2365), tensor(-0.14...</td>\n",
       "      <td>a bright landscape</td>\n",
       "      <td>0.127563</td>\n",
       "      <td>0.066242</td>\n",
       "      <td>0.551611</td>\n",
       "      <td>geleje</td>\n",
       "      <td>99.289937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11995</th>\n",
       "      <td>validation</td>\n",
       "      <td>Testeregitar_-_F.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Testeregitar_-_...</td>\n",
       "      <td>[[tensor(834.), tensor(604.), tensor(432.), te...</td>\n",
       "      <td>Mountain-Valid (95).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(-0.1323), tensor(0.4341), tensor(0.136...</td>\n",
       "      <td>a intense landscape</td>\n",
       "      <td>0.072266</td>\n",
       "      <td>0.040474</td>\n",
       "      <td>0.544531</td>\n",
       "      <td>juxuluji</td>\n",
       "      <td>98.015541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11996</th>\n",
       "      <td>validation</td>\n",
       "      <td>Anvil_-_Lokomo_A_100_kg_-_Hammer_on_back_1_tim...</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Anvil_-_Lokomo_...</td>\n",
       "      <td>[[tensor(170.), tensor(748.), tensor(748.), te...</td>\n",
       "      <td>Mountain-Valid (96).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(-0.3210), tensor(0.1672), tensor(0.141...</td>\n",
       "      <td>a complex landscape</td>\n",
       "      <td>0.069275</td>\n",
       "      <td>0.056088</td>\n",
       "      <td>0.512671</td>\n",
       "      <td>juxuluja</td>\n",
       "      <td>92.280799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11997</th>\n",
       "      <td>validation</td>\n",
       "      <td>Short_Cut-Off_Beep.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/Short_Cut-Off_B...</td>\n",
       "      <td>[[tensor(103.), tensor(56.), tensor(0.), tenso...</td>\n",
       "      <td>Mountain-Valid (97).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(0.3450), tensor(0.3041), tensor(0.0990...</td>\n",
       "      <td>a narrow landscape</td>\n",
       "      <td>0.117004</td>\n",
       "      <td>0.043641</td>\n",
       "      <td>0.532324</td>\n",
       "      <td>juxuluji</td>\n",
       "      <td>95.818241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11998</th>\n",
       "      <td>validation</td>\n",
       "      <td>BS_Bend_17.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/BS_Bend_17.wav</td>\n",
       "      <td>[[tensor(319.), tensor(698.), tensor(857.), te...</td>\n",
       "      <td>Mountain-Valid (98).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(0.1858), tensor(-0.0550), tensor(-0.14...</td>\n",
       "      <td>a sharp landscape</td>\n",
       "      <td>0.099854</td>\n",
       "      <td>0.048247</td>\n",
       "      <td>0.554051</td>\n",
       "      <td>gelejegi</td>\n",
       "      <td>99.729109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>validation</td>\n",
       "      <td>01530_censore_beep_05s.wav</td>\n",
       "      <td>dataset/earcon_dataset/earcons/01530_censore_b...</td>\n",
       "      <td>[[tensor(1009.), tensor(205.), tensor(606.), t...</td>\n",
       "      <td>Mountain-Valid (99).jpeg</td>\n",
       "      <td>dataset/landscape1/Validation Data/Mountain\\Mo...</td>\n",
       "      <td>[tensor(0.0373), tensor(-0.1407), tensor(-0.11...</td>\n",
       "      <td>a positive landscape</td>\n",
       "      <td>0.071838</td>\n",
       "      <td>0.049314</td>\n",
       "      <td>0.565472</td>\n",
       "      <td>gelejegi</td>\n",
       "      <td>101.785030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12000 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            split                                    earcon_filename  \\\n",
       "0            test                                     BS_Bend_20.wav   \n",
       "1            test                                     Failure_01.wav   \n",
       "2            test                                     BS_Bend_17.wav   \n",
       "3            test                                          clock.wav   \n",
       "4            test                                     Failure_01.wav   \n",
       "...           ...                                                ...   \n",
       "11995  validation                               Testeregitar_-_F.wav   \n",
       "11996  validation  Anvil_-_Lokomo_A_100_kg_-_Hammer_on_back_1_tim...   \n",
       "11997  validation                             Short_Cut-Off_Beep.wav   \n",
       "11998  validation                                     BS_Bend_17.wav   \n",
       "11999  validation                         01530_censore_beep_05s.wav   \n",
       "\n",
       "                                         earcon_filepath  \\\n",
       "0          dataset/earcon_dataset/earcons/BS_Bend_20.wav   \n",
       "1          dataset/earcon_dataset/earcons/Failure_01.wav   \n",
       "2          dataset/earcon_dataset/earcons/BS_Bend_17.wav   \n",
       "3               dataset/earcon_dataset/earcons/clock.wav   \n",
       "4          dataset/earcon_dataset/earcons/Failure_01.wav   \n",
       "...                                                  ...   \n",
       "11995  dataset/earcon_dataset/earcons/Testeregitar_-_...   \n",
       "11996  dataset/earcon_dataset/earcons/Anvil_-_Lokomo_...   \n",
       "11997  dataset/earcon_dataset/earcons/Short_Cut-Off_B...   \n",
       "11998      dataset/earcon_dataset/earcons/BS_Bend_17.wav   \n",
       "11999  dataset/earcon_dataset/earcons/01530_censore_b...   \n",
       "\n",
       "                                         earcon_features  \\\n",
       "0      [[tensor(930.), tensor(534.), tensor(530.), te...   \n",
       "1      [[tensor(913.), tensor(945.), tensor(530.), te...   \n",
       "2      [[tensor(319.), tensor(698.), tensor(857.), te...   \n",
       "3      [[tensor(62.), tensor(62.), tensor(62.), tenso...   \n",
       "4      [[tensor(913.), tensor(945.), tensor(530.), te...   \n",
       "...                                                  ...   \n",
       "11995  [[tensor(834.), tensor(604.), tensor(432.), te...   \n",
       "11996  [[tensor(170.), tensor(748.), tensor(748.), te...   \n",
       "11997  [[tensor(103.), tensor(56.), tensor(0.), tenso...   \n",
       "11998  [[tensor(319.), tensor(698.), tensor(857.), te...   \n",
       "11999  [[tensor(1009.), tensor(205.), tensor(606.), t...   \n",
       "\n",
       "                 image_filename  \\\n",
       "0           Coast-Test (1).jpeg   \n",
       "1         Coast-Test (100).jpeg   \n",
       "2         Coast-Test (101).jpeg   \n",
       "3         Coast-Test (102).jpeg   \n",
       "4         Coast-Test (103).jpeg   \n",
       "...                         ...   \n",
       "11995  Mountain-Valid (95).jpeg   \n",
       "11996  Mountain-Valid (96).jpeg   \n",
       "11997  Mountain-Valid (97).jpeg   \n",
       "11998  Mountain-Valid (98).jpeg   \n",
       "11999  Mountain-Valid (99).jpeg   \n",
       "\n",
       "                                          image_filepath  \\\n",
       "0      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "1      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "2      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "3      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "4      dataset/landscape1/Testing Data/Coast\\Coast-Te...   \n",
       "...                                                  ...   \n",
       "11995  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11996  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11997  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11998  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "11999  dataset/landscape1/Validation Data/Mountain\\Mo...   \n",
       "\n",
       "                                          image_features  \\\n",
       "0      [tensor(0.5427), tensor(-0.2114), tensor(-0.54...   \n",
       "1      [tensor(0.1623), tensor(-0.0117), tensor(-0.17...   \n",
       "2      [tensor(0.5334), tensor(-0.1099), tensor(-0.36...   \n",
       "3      [tensor(0.8153), tensor(-0.2710), tensor(0.351...   \n",
       "4      [tensor(0.3290), tensor(-0.2365), tensor(-0.14...   \n",
       "...                                                  ...   \n",
       "11995  [tensor(-0.1323), tensor(0.4341), tensor(0.136...   \n",
       "11996  [tensor(-0.3210), tensor(0.1672), tensor(0.141...   \n",
       "11997  [tensor(0.3450), tensor(0.3041), tensor(0.0990...   \n",
       "11998  [tensor(0.1858), tensor(-0.0550), tensor(-0.14...   \n",
       "11999  [tensor(0.0373), tensor(-0.1407), tensor(-0.11...   \n",
       "\n",
       "                  image_tag  image_tag_similarity  similarity_score  \\\n",
       "0       a natural landscape              0.098083          0.059398   \n",
       "1       a natural landscape              0.120972          0.057352   \n",
       "2          a calm landscape              0.133911          0.072991   \n",
       "3        a bright landscape              0.113892          0.056997   \n",
       "4        a bright landscape              0.127563          0.066242   \n",
       "...                     ...                   ...               ...   \n",
       "11995   a intense landscape              0.072266          0.040474   \n",
       "11996   a complex landscape              0.069275          0.056088   \n",
       "11997    a narrow landscape              0.117004          0.043641   \n",
       "11998     a sharp landscape              0.099854          0.048247   \n",
       "11999  a positive landscape              0.071838          0.049314   \n",
       "\n",
       "       roundness pseudoword  weighted_angles  \n",
       "0       0.548406   juxuluji        98.713105  \n",
       "1       0.515597     juxulu        92.807444  \n",
       "2       0.562980     geleje       101.336326  \n",
       "3       0.533194     juxulu        95.974916  \n",
       "4       0.551611     geleje        99.289937  \n",
       "...          ...        ...              ...  \n",
       "11995   0.544531   juxuluji        98.015541  \n",
       "11996   0.512671   juxuluja        92.280799  \n",
       "11997   0.532324   juxuluji        95.818241  \n",
       "11998   0.554051   gelejegi        99.729109  \n",
       "11999   0.565472   gelejegi       101.785030  \n",
       "\n",
       "[12000 rows x 13 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "earcon_image_dataset = pd.read_pickle('dataset/combined_dataset/earcon_image_dataset2.pkl')\n",
    "earcon_image_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset based on the \"split\" column\n",
    "train_df = earcon_image_dataset[earcon_image_dataset['split'] == 'train']\n",
    "train_df = train_df.drop(columns='split')\n",
    "val_df = earcon_image_dataset[earcon_image_dataset['split'] == 'validation']\n",
    "val_df = val_df.drop(columns='split')\n",
    "test_df = earcon_image_dataset[earcon_image_dataset['split'] == 'test']\n",
    "test_df = test_df.drop(columns='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 75, 500)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=0.01, random_state=42)\n",
    "val_df = val_df.sample(frac=0.05, random_state=42)\n",
    "test_df = test_df.sample(frac=1, random_state=42)\n",
    "len(train_df), len(val_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "from utils.musicgen_utils import create_earcon_dataloaders\n",
    "train_loader, val_loader, test_loader = create_earcon_dataloaders(train_df, val_df, test_df, batch_size=1, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\transformers\\models\\musicgen\\modeling_musicgen.py:539: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Epoch 1/1:   0%|          | 0/100 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain_earcon_generation_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMusicGenModel_V2_01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\OneDrive - Nanyang Technological University\\FYP\\FYP\\utils\\musicgen_utils.py:195\u001b[0m, in \u001b[0;36mtrain_earcon_generation_model\u001b[1;34m(model, train_dataloader, val_dataloader, optimizer, device, num_epochs, save_path, version)\u001b[0m\n\u001b[0;32m    189\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m    190\u001b[0m     image_features,\n\u001b[0;32m    191\u001b[0m     roundness,\n\u001b[0;32m    192\u001b[0m )\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m--> 195\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mearcon_generation_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearcon_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mroundness\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m    203\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mf:\\OneDrive - Nanyang Technological University\\FYP\\FYP\\utils\\musicgen_model.py:244\u001b[0m, in \u001b[0;36mearcon_generation_loss\u001b[1;34m(generated_audio, target_audio, image_features, roundness_values)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03mCustom loss function for earcon generation\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Computed loss\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# 1. Reconstruction Loss\u001b[39;00m\n\u001b[1;32m--> 244\u001b[0m reconstruction_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# 2. Feature Consistency Loss\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# Ensure generated audio maintains some relationship with input features\u001b[39;00m\n\u001b[0;32m    248\u001b[0m image_consistency_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcosine_similarity(\n\u001b[0;32m    249\u001b[0m     image_features,\n\u001b[0;32m    250\u001b[0m     generated_audio\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# Aggregate audio features\u001b[39;00m\n\u001b[0;32m    251\u001b[0m )\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\qkm20\\anaconda3\\envs\\fyp\\Lib\\site-packages\\torch\\nn\\functional.py:3373\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[0;32m   3370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   3371\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[0;32m   3372\u001b[0m     )\n\u001b[1;32m-> 3373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()):\n\u001b[0;32m   3374\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3375\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3376\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3377\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3378\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   3379\u001b[0m     )\n\u001b[0;32m   3380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Train the model\n",
    "train_earcon_generation_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=1,\n",
    "    save_path='outputs/',\n",
    "    version=\"MusicGenModel_V2_01\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "from utils.musicgen_model import *\n",
    "\n",
    "\n",
    "model = load_model(\"outputs/MusicGenModel_V2_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# choose 10 unique random image/earcon pairs for testing\n",
    "elements = random.sample(range(0, len(test_df)), 10)\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the audio for the selected pairs\n",
    "\n",
    "audio_list = []\n",
    "\n",
    "for i in range(len(elements)):\n",
    "    audio_list.append(\n",
    "        generate_earcon(\n",
    "            model,\n",
    "            test_df[\"image_vector\"].iloc[elements[i]],\n",
    "            test_df[\"roundness\"].iloc[elements[i]]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display, Image\n",
    "\n",
    "\n",
    "def display_image(image_path):\n",
    "    display(Image(filename=image_path))\n",
    "\n",
    "\n",
    "def play_audio(filepath):\n",
    "    display(Audio(filepath))\n",
    "\n",
    "\n",
    "def play_generated_audio(audio, sampling_rate=24000):\n",
    "    display(Audio(audio[0].cpu().numpy(), rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the images and play the audio for every pair\n",
    "for i in range(len(elements)):\n",
    "    # display image\n",
    "    print(f\"Image {i+1}:\")\n",
    "    display_image(test_df[\"image\"].iloc[elements[i]])\n",
    "    # play paired earcon\n",
    "    print(f\"Original Paired Earcon {i+1}:\")\n",
    "    play_audio(test_df[\"earcon\"].iloc[elements[i]])\n",
    "    # play generated earcon\n",
    "    print(f\"Generated Earcon {i+1}:\")\n",
    "    play_generated_audio(audio_list[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

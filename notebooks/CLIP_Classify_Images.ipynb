{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# summary\n",
    "\n",
    "this notebook is built to classify all the images in landscape1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports\n",
    "\n",
    "this section imports all relevant libraries, the CLIP model, and the earcon tags from the earcon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "import ast\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in earcon dataset\n",
    "\n",
    "if os.path.isfile(\"../dataset/earcon_dataset/earcon_dataset.csv\"):\n",
    "    earcon_dataset = pd.read_excel('../dataset/earcon_dataset/earcon_dataset.csv')\n",
    "\n",
    "earcon_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earcon_tags = earcon_dataset['tags'].tolist()\n",
    "\n",
    "taglist = []\n",
    "for element in earcon_tags:\n",
    "    temp = element.replace(\"[\", \"\")\n",
    "    temp = temp.replace(\"]\", \"\")\n",
    "    temp = temp.replace(\"'\", \"\")\n",
    "    temp = temp.split(\", \")\n",
    "    # print(temp)\n",
    "    for tag in temp:\n",
    "        tag = tag.strip()\n",
    "        tag = tag.lower()\n",
    "        if tag not in taglist:\n",
    "            taglist.append(tag)\n",
    "\n",
    "print(f\"There are {len(taglist)} unique tags in the dataset\")\n",
    "print(f\"Here are the first 10 tags: {taglist[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(taglist)-1, 0, -1):\n",
    "    if len(taglist[i]) == 1:\n",
    "        taglist.pop(i)\n",
    "        \n",
    "# print(f\"There are {len(taglist)} unique tags in the dataset\")\n",
    "# print(f\"Here are the first 10 tags: {taglist[:10]}\")\n",
    "for tag in taglist:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual tags\n",
    "\n",
    "taglist = [\n",
    "    \"bright\", \"dark\", \"happy\", \"sad\", \"excited\", \"calm\", \"fast\", \"slow\", \"cold\", \"warm\", \"loud\", \"quiet\", \"dark\", \"light\",\n",
    "    \"dull\", \"sharp\", \"flat\", \"low\", \"high\", \"intense\", \"soft\", \"rough\", \"sparkling\", \"simple\", \"complex\", \"natural\",\n",
    "    \"artificial\", \"clean\", \"horror\", \"scary\", \"mysterious\", \"correct\", \"incorrect\", \"accept\", \"reject\", \"agree\", \"disagree\",\n",
    "    \"menu\", \"analog\", \"digital\", \"positive\", \"negative\", \"good\", \"bad\", \"win\", \"lose\", \"start\", \"stop\", \"yes\", \"no\",\n",
    "    \"curved\", \"straight\", \"open\", \"closed\", \"up\", \"down\", \"left\", \"right\", \"long\", \"short\", \"shrill\", \"deep\", \"narrow\",\n",
    "    \"musical\", \"nonmusical\", \"melodic\", \"rhythmic\", \"harmonic\", \"dissonant\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taglist1 = []\n",
    "taglist2 = []\n",
    "taglist3 = []\n",
    "taglist4 = []\n",
    "taglist5 = []\n",
    "\n",
    "for i in range(len(taglist)):\n",
    "    taglist1.append(f\"a {taglist[i]} landscape\")\n",
    "    taglist2.append(f\"a landscape picture with a {taglist[i]} scene\")\n",
    "    taglist3.append(f\"a {taglist[i]} landscape picture\")\n",
    "    taglist4.append(f\"this picture gives a {taglist[i]} feeling\")\n",
    "    taglist5.append(f\"this picture is {taglist[i]}\")\n",
    "\n",
    "taglist1_tokens = clip.tokenize(taglist1).to(device)\n",
    "taglist2_tokens = clip.tokenize(taglist2).to(device)\n",
    "taglist3_tokens = clip.tokenize(taglist3).to(device)\n",
    "taglist4_tokens = clip.tokenize(taglist4).to(device)\n",
    "taglist5_tokens = clip.tokenize(taglist5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load paths\n",
    "\n",
    "this section crawls all the images so that we have a list of image files that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the code using validation coast from landscape dataset 1\n",
    "\n",
    "sub_folders = [\n",
    "    \"Coast\",\n",
    "    \"Desert\",\n",
    "    \"Forest\",\n",
    "    \"Glacier\",\n",
    "    \"Mountain\"\n",
    "]\n",
    "\n",
    "split = [\n",
    "    \"test\",\n",
    "    \"train\",\n",
    "    \"validation\"\n",
    "]\n",
    "\n",
    "folders = [\n",
    "    \"../dataset/landscape1/Testing Data/\",\n",
    "    \"../dataset/landscape1/Training Data/\",\n",
    "    \"../dataset/landscape1/Validation Data/\",\n",
    "]\n",
    "\n",
    "image_paths = []\n",
    "\n",
    "# for filepath in folders:\n",
    "for i in range(len(folders)):\n",
    "    for folder in sub_folders:\n",
    "        for item in os.scandir(folders[i] + folder):\n",
    "            image_paths.append({\"split\": split[i], \"folder\": folder, \"filename\": item.name, \"filepath\": item.path})\n",
    "\n",
    "image_paths = pd.DataFrame(image_paths)\n",
    "image_paths.to_csv(\"../dataset/landscape1/csvs/image_paths.csv\", index=False)\n",
    "image_paths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image_df, tags, text_tokens, k=10):\n",
    "    result = []\n",
    "    splits = image_df[\"split\"].tolist()\n",
    "    folders = image_df[\"folder\"].tolist()\n",
    "    filenames = image_df[\"filename\"].tolist()\n",
    "    paths = image_df[\"filepath\"].tolist()\n",
    "\n",
    "    # Forward pass for each image\n",
    "    for i in range(len(image_paths)):\n",
    "        # Load and preprocess the image\n",
    "        image = preprocess(Image.open(paths[i])).unsqueeze(0).to(device)\n",
    "\n",
    "        # Forward pass to get image and text features\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            text_features = model.encode_text(text_tokens)\n",
    "\n",
    "        # Normalize features to compare cosine similarity\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Compute similarity between the image and text prompts\n",
    "        similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "        # Get the top k predictions (tags and similarity scores)\n",
    "        top_preds = torch.topk(similarities, k)\n",
    "        top_indices = top_preds.indices.squeeze(0).tolist()\n",
    "        top_scores = top_preds.values.squeeze(0).tolist()\n",
    "\n",
    "        # Create a dictionary entry for the image classification\n",
    "        result.append({\n",
    "            \"split\": splits[i],\n",
    "            \"folder\": folders[i],\n",
    "            \"filename\": filenames[i],\n",
    "            \"image_path\": paths[i],\n",
    "            \"top_tags\": [tags[i] for i in top_indices],\n",
    "            \"similarity_scores\": top_scores\n",
    "        })\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Processed {i+1}/{len(paths)} images\")\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    result = pd.DataFrame(result)\n",
    "    result[\"top_tags\"] = result[\"top_tags\"].apply(lambda x: str(x))\n",
    "    result[\"similarity_scores\"] = result[\"similarity_scores\"].apply(lambda x: str(x))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = inference(image_paths, taglist1, taglist1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = inference(image_paths, taglist2, taglist2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = inference(image_paths, taglist3, taglist3_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = inference(image_paths, taglist4, taglist4_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result5 = inference(image_paths, taglist5, taglist5_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# verify images and tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_results(result, n=10):\n",
    "    temp = result.sample(n)\n",
    "    for index, row in temp.iterrows():\n",
    "        # Display the image\n",
    "        image = Image.open(row['image_path'])\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        tags = ast.literal_eval(row[\"top_tags\"])\n",
    "        scores = ast.literal_eval(row[\"similarity_scores\"])\n",
    "\n",
    "        # Display the top n tags and their similarity scores\n",
    "        count = 1\n",
    "        print(f\"Top {n} Tags and Similarity Scores:\")\n",
    "        for tag, score in zip(tags[:n], scores[:n]):\n",
    "            print(f\"{count:>2}. {tag:>50}: {score:.2f}\")\n",
    "            count += 1\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_results(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_results(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_results(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_results(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_results(result5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
